training_parameters:
  total_timesteps: 100000

  ppo_kwargs:
    learning_rate: 0.0005
    n_steps: 2048
    batch_size: 64
    n_epochs: 50
    gamma: 0.99
    gae_lambda: 0.95
    clip_range: 0.2
    ent_coef: 0.0
    vf_coef: 0.5
    max_grad_norm: 0.5
    use_sde: False
    policy_kwargs:
      net_arch: [256, 256]

  reward_function:
    raise: 1.0
    cost: true
    cost_scale: 0.01

configuration: "PPO"
configuration_save_name: "ppo"
